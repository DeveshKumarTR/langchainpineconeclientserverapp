# LangChain Pinecone Vector Database Application

A Flask-based client-server application using LangChain and Pinecone vector database for document embedding, storage, and semantic retrieval.

## âœ¨ Features

- **Document Processing**: Upload and process documents (PDF, TXT, DOCX, XLSX) using LangChain
- **Vector Storage**: Store document embeddings in Pinecone vector database
- **Semantic Search**: Search documents using vector similarity
- **REST API**: Flask-based API for client-server communication
- **Interactive Client**: Python client for easy interaction with the server
- **Batch Operations**: Support for bulk document processing
- **Similarity Analysis**: Find documents similar to a given document

## ğŸ—ï¸ Project Structure

```
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ app.py              # Flask server application
â”‚   â”œâ”€â”€ config.py           # Configuration settings
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ vector_store.py # Pinecone vector store implementation
â”‚   â””â”€â”€ routes/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ documents.py    # Document upload/processing endpoints
â”‚       â””â”€â”€ search.py       # Search endpoints
â”œâ”€â”€ client/
â”‚   â”œâ”€â”€ client.py           # Client application
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ helpers.py      # Client utility functions
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py         # Global configuration
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_server.py      # Server tests
â”‚   â””â”€â”€ test_client.py      # Client tests
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ README.md           # Usage examples and scripts
â”œâ”€â”€ requirements.txt        # Project dependencies
â”œâ”€â”€ .env.example           # Environment variables template
â”œâ”€â”€ INSTALL.md             # Detailed setup instructions
â””â”€â”€ README.md              # This file
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+ installed on your system
- Pinecone API key ([Get one here](https://www.pinecone.io/))
- OpenAI API key ([Get one here](https://platform.openai.com/api-keys))

### 1. Setup Environment
```bash
# Install Python dependencies
pip install -r requirements.txt

# Configure environment variables
copy .env.example .env
# Edit .env with your API keys
```

### 2. Initialize Vector Database
```bash
# Initialize Pinecone index
python -m flask --app server.app init-db
```

### 3. Start the Server
```bash
python server/app.py
```

### 4. Use the Client
```bash
# In a new terminal
python client/client.py
```

## ğŸ“‹ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/health` | Health check |
| `POST` | `/api/documents` | Upload and process documents |
| `GET` | `/api/documents` | List processed documents |
| `DELETE` | `/api/documents/{id}` | Delete a document |
| `POST` | `/api/search` | Search documents by query |
| `POST` | `/api/search/similar` | Find similar documents |
| `GET` | `/api/search/stats` | Get vector store statistics |

## ğŸ”§ Configuration

Key environment variables:

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `PINECONE_API_KEY` | âœ… | - | Your Pinecone API key |
| `OPENAI_API_KEY` | âœ… | - | Your OpenAI API key |
| `PINECONE_INDEX_NAME` | âŒ | `langchain-documents` | Pinecone index name |
| `CHUNK_SIZE` | âŒ | `1000` | Text chunk size for processing |
| `CHUNK_OVERLAP` | âŒ | `200` | Overlap between text chunks |
| `FLASK_PORT` | âŒ | `5000` | Server port |

## ğŸ“– Usage Examples

### Upload a Document
```python
from client.client import LangChainPineconeClient

client = LangChainPineconeClient()
result = client.upload_document("path/to/document.pdf")
print(result)
```

### Search Documents
```python
results = client.search_documents("machine learning", k=5)
for result in results['results']:
    print(f"Score: {result['similarity_score']:.3f}")
    print(f"Content: {result['content'][:100]}...")
```

### API Usage with cURL
```bash
# Upload document
curl -X POST -F "file=@document.pdf" http://127.0.0.1:5000/api/documents

# Search documents
curl -X POST -H "Content-Type: application/json" \
  -d '{"query": "artificial intelligence", "k": 5}' \
  http://127.0.0.1:5000/api/search
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest tests/ --cov=server --cov=client
```

## ğŸ“ Supported File Types

- **PDF** (.pdf) - Portable Document Format
- **Text** (.txt) - Plain text files
- **Word** (.docx) - Microsoft Word documents
- **Excel** (.xlsx) - Microsoft Excel spreadsheets

## ğŸ› ï¸ Development

### Code Formatting
```bash
black .
flake8 .
```

### Adding New Features
1. Add endpoints in `server/routes/`
2. Update the vector store model in `server/models/vector_store.py`
3. Add client methods in `client/client.py`
4. Write tests in `tests/`

## ğŸ“š Documentation

- **[Installation Guide](INSTALL.md)** - Detailed setup instructions
- **[Examples](examples/README.md)** - Usage examples and scripts
- **[API Documentation](#-api-endpoints)** - REST API reference

## ğŸ” Troubleshooting

### Common Issues

**Python not found**: Install Python 3.8+ from [python.org](https://www.python.org/downloads/)

**API key errors**: Check your `.env` file configuration

**Import errors**: Ensure all dependencies are installed with `pip install -r requirements.txt`

**Pinecone connection issues**: Verify your API key and index name

See [INSTALL.md](INSTALL.md) for detailed troubleshooting guide.

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

---

**Built with**: Flask, LangChain, Pinecone, OpenAI
